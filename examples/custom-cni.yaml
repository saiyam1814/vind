# vind Cluster with Custom CNI Configuration
# This example shows how to disable the default Flannel CNI and install a custom CNI plugin
#
# IMPORTANT NOTES:
# 1. When using the Docker driver, vCluster automatically enables private nodes mode.
#    This is expected behavior and required for the Docker driver to work correctly.
# 2. In Docker mode, you MUST define worker nodes in experimental.docker.nodes.
#    Without nodes defined, kubectl get nodes will return "No resources found".
# 3. Without a CNI plugin, pods will be stuck in Pending state.

experimental:
  docker:
    # Load balancer and registry proxy are enabled by default
    
    # IMPORTANT: Define at least one worker node
    # Without nodes, kubectl get nodes will return nothing
    nodes:
      - name: worker-1

# CNI Configuration
# vCluster only supports Flannel natively. To use other CNI plugins (Calico, Cilium, etc.):
# 1. Disable Flannel (set enabled: false)
# 2. Create the cluster
# 3. Manually install your preferred CNI plugin IMMEDIATELY after cluster creation
#    (pods will be stuck in Pending state until CNI is installed)

deploy:
  cni:
    flannel:
      enabled: false  # Disable default Flannel CNI

# Kubernetes version (optional, defaults to v1.34.0)
controlPlane:
  distro:
    k8s:
      version: "v1.34.0"

# Create the cluster with:
# vcluster create my-cni-cluster -f custom-cni.yaml

# Verify nodes are created (you should see worker-1):
# kubectl get nodes

# IMPORTANT: Install your preferred CNI plugin IMMEDIATELY after cluster creation.
# Without CNI, pods will be stuck in Pending state.

# For Calico:
# kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.1/manifests/tigera-operator.yaml
# kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.1/manifests/custom-resources.yaml

# For Cilium:
# helm repo add cilium https://helm.cilium.io/
# helm install cilium cilium/cilium --version 1.14.0

# After CNI is installed, wait for pods to become Ready:
# kubectl get pods --all-namespaces -w

# Note: Make sure to configure the CNI plugin according to your cluster's pod CIDR
# Check the pod CIDR: kubectl cluster-info dump | grep -m 1 cluster-cidr
